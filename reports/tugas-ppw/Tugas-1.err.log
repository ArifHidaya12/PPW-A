Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/jupyter_cache/executors/utils.py", line 58, in single_nb_execution
    executenb(
  File "/usr/local/lib/python3.10/dist-packages/nbclient/client.py", line 1314, in execute
    return NotebookClient(nb=nb, resources=resources, km=km, **kwargs).execute()
  File "/usr/local/lib/python3.10/dist-packages/jupyter_core/utils/__init__.py", line 165, in wrapped
    return loop.run_until_complete(inner)
  File "/usr/lib/python3.10/asyncio/base_events.py", line 649, in run_until_complete
    return future.result()
  File "/usr/local/lib/python3.10/dist-packages/nbclient/client.py", line 709, in async_execute
    await self.async_execute_cell(
  File "/usr/local/lib/python3.10/dist-packages/nbclient/client.py", line 1062, in async_execute_cell
    await self._check_raise_for_error(cell, cell_index, exec_reply)
  File "/usr/local/lib/python3.10/dist-packages/nbclient/client.py", line 918, in _check_raise_for_error
    raise CellExecutionError.from_cell_and_msg(cell, exec_reply_content)
nbclient.exceptions.CellExecutionError: An error occurred while executing the following cell:
------------------
import requests
from bs4 import BeautifulSoup
import pandas as pd
import time

# Fungsi untuk mengambil data dari halaman web Detik.com
def get_data(url, kategori, min_articles_per_category):
    try:
        response = requests.get(url)
        response.raise_for_status()
    except requests.exceptions.RequestException as e:
        print(f"Request failed: {e}")
        return

    soup = BeautifulSoup(response.content, "html.parser")
    articles = soup.find_all("article", class_="list-content__item")

    for article in articles:
        if len([k for k in kategori_list if k == kategori]) >= min_articles_per_category:
            return  # Menghentikan proses jika jumlah artikel sudah mencapai minimum yang diinginkan

        try:
            link = article.find("a")["href"]
            article_response = requests.get(link)
            article_response.raise_for_status()
        except (requests.exceptions.RequestException, TypeError) as e:
            print(f"Request for article failed: {e}")
            continue

        article_soup = BeautifulSoup(article_response.content, "html.parser")
        title_element = article_soup.find("h1", class_="detail__title")
        title = title_element.text.strip() if title_element else "Title Not Found"
        date_element = article_soup.find("div", class_="detail__date")
        date = date_element.text.strip() if date_element else "Date Not Found"
        content_element = article_soup.find("div", class_="detail__body-text")
        content = content_element.text.strip() if content_element else "Content Not Found"

        judul.append(title)
        tanggal.append(date)
        isi.append(content)
        url_list.append(link)
        kategori_list.append(kategori)

        print(title)
        time.sleep(1)  # Menambahkan jeda waktu 1 detik antara permintaan artikel

# Membuat list url dan kategori yang akan di-crawl
base_urls = [
    "https://health.detik.com/berita-detikhealth/indeks",
    "https://travel.detik.com/travel-news/indeks"
]
categories = [
    "Kesehatan",
    "Pariwisata"
]

# Inisialisasi list untuk menyimpan data
judul = []
tanggal = []
isi = []
kategori_list = []

# Batas minimal artikel per kategori
min_articles_per_category = 50

# Melakukan iterasi untuk setiap url dan kategori
for base_url, category in zip(base_urls, categories):
    page = 1
    while len([k for k in kategori_list if k == category]) < min_articles_per_category:
        url = f"{base_url}/{page}"
        get_data(url, category, min_articles_per_category)
        time.sleep(2)  # Menambahkan jeda waktu 2 detik antara permintaan halaman
        page += 1

# Membuat dataframe dari list data
df = pd.DataFrame({"judul": judul, "tanggal": tanggal, "isi": isi, "kategori": kategori_list})

# Menyimpan dataframe ke file csv
df.to_csv("data_berita_detik.csv", index=False)

# Menampilkan dataframe
print(df)
------------------


[0;31m---------------------------------------------------------------------------[0m
[0;31mNameError[0m                                 Traceback (most recent call last)
[0;32m<ipython-input-1-483dee741f7b>[0m in [0;36m<cell line: 67>[0;34m()[0m
[1;32m     69[0m     [0;32mwhile[0m [0mlen[0m[0;34m([0m[0;34m[[0m[0mk[0m [0;32mfor[0m [0mk[0m [0;32min[0m [0mkategori_list[0m [0;32mif[0m [0mk[0m [0;34m==[0m [0mcategory[0m[0;34m][0m[0;34m)[0m [0;34m<[0m [0mmin_articles_per_category[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[1;32m     70[0m         [0murl[0m [0;34m=[0m [0;34mf"{base_url}/{page}"[0m[0;34m[0m[0;34m[0m[0m
[0;32m---> 71[0;31m         [0mget_data[0m[0;34m([0m[0murl[0m[0;34m,[0m [0mcategory[0m[0;34m,[0m [0mmin_articles_per_category[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m     72[0m         [0mtime[0m[0;34m.[0m[0msleep[0m[0;34m([0m[0;36m2[0m[0;34m)[0m  [0;31m# Menambahkan jeda waktu 2 detik antara permintaan halaman[0m[0;34m[0m[0;34m[0m[0m
[1;32m     73[0m         [0mpage[0m [0;34m+=[0m [0;36m1[0m[0;34m[0m[0;34m[0m[0m

[0;32m<ipython-input-1-483dee741f7b>[0m in [0;36mget_data[0;34m(url, kategori, min_articles_per_category)[0m
[1;32m     39[0m         [0mtanggal[0m[0;34m.[0m[0mappend[0m[0;34m([0m[0mdate[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[1;32m     40[0m         [0misi[0m[0;34m.[0m[0mappend[0m[0;34m([0m[0mcontent[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0;32m---> 41[0;31m         [0murl_list[0m[0;34m.[0m[0mappend[0m[0;34m([0m[0mlink[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m     42[0m         [0mkategori_list[0m[0;34m.[0m[0mappend[0m[0;34m([0m[0mkategori[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[1;32m     43[0m [0;34m[0m[0m

[0;31mNameError[0m: name 'url_list' is not defined

